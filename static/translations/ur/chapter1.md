---
sidebar_position: 2
---

# باب 1: روبوٹ سینسرز اور ادراک

ماحول کو سمجھنا کسی بھی ذہین روبوٹ کے لیے پہلا قدم ہے۔ اس باب میں، ہم حسی نظاموں کا جائزہ لیتے ہیں جو روبوٹس کو اپنے ارد گرد کی جسمانی دنیا کو سمجھنے اور تشریح کرنے کے قابل بناتے ہیں۔

## جائزہ

روبوٹ ادراک حسی ڈیٹا حاصل کرنے، پروسیس کرنے، اور تشریح کرنے کا عمل ہے تاکہ ماحول کی نمائندگی بنائی جا سکے۔ ہیومنائڈ روبوٹس کے لیے، ادراک کے نظاموں کو لازمی طور پر:

- **تیز**: متحرک ماحول کے لیے ریئل ٹائم پروسیسنگ
- **مضبوط**: مختلف حالات میں قابل اعتماد (روشنی، موسم، رکاوٹ)
- **ملٹی موڈل**: جامع سمجھ کے لیے متعدد سینسر اقسام کو یکجا کرنا
- **موثر**: کمپیوٹیشنل اور پاور رکاوٹوں کے اندر کام کرنا

## بصری ادراک کے نظام

بصارت یقیناً ہیومنائڈ روبوٹس کے لیے سب سے اہم حس ہے، جو ماحول کے بارے میں بھرپور معلومات فراہم کرتی ہے۔

### کیمرہ ٹیکنالوجیز

**مونوکیولر کیمرے**
- 2D تصاویر فراہم کرنے والا واحد کیمرہ
- ہلکا وزن اور سستا
- گہرائی کی معلومات کے لیے سیکھنے یا حرکت کی ضرورت

**سٹیریو کیمرے**
- انسانی دوہری بصارت کی نقل کرنے والے دو کیمرے
- تثلیث کے ذریعے براہ راست گہرائی کا تخمینہ
- محدود رینج اور کیلیبریشن کی ضرورت

**RGB-D کیمرے**
- رنگ اور گہرائی کی معلومات کو یکجا کرتے ہیں
- ایکٹو سینسنگ (structured light یا time-of-flight)
- مثالیں: Microsoft Kinect, Intel RealSense, Apple LiDAR

**ایونٹ کیمرے**
- پکسل لیول کی چمک میں تبدیلیوں کا پتہ لگانے والے نیورومورفک سینسرز
- انتہائی کم لیٹینسی (1ms سے کم)
- تیز رفتار حرکت اور کم روشنی کے حالات کے لیے بہترین

### کمپیوٹر ویژن الگورتھمز

جدید روبوٹ ویژن ڈیپ لرننگ کا وسیع استعمال کرتا ہے:

**آبجیکٹ ڈیٹیکشن**
```python
# مثال: ریئل ٹائم آبجیکٹ ڈیٹیکشن کے لیے YOLO کا استعمال
import cv2
from ultralytics import YOLO

# پری ٹرینڈ ماڈل لوڈ کریں
model = YOLO('yolov8n.pt')

# کیمرہ فریم پروسیس کریں
def detect_objects(frame):
    results = model(frame)
    
    for result in results:
        boxes = result.boxes
        for box in boxes:
            # باؤنڈنگ باکس کوآرڈینیٹس نکالیں
            x1, y1, x2, y2 = box.xyxy[0]
            confidence = box.conf[0]
            class_id = box.cls[0]
            
            # ڈیٹیکشن ڈرا کریں
            cv2.rectangle(frame, (int(x1), int(y1)), 
                         (int(x2), int(y2)), (0, 255, 0), 2)
    
    return frame
```

**سیمینٹک سیگمینٹیشن**
- سین عناصر کی پکسل وائز کلاسیفیکیشن
- نیویگیشن اور ہیرا پھیری کے لیے ضروری
- جدید آرکیٹیکچرز: DeepLab, Mask R-CNN, Segment Anything Model (SAM)

**پوز ایسٹیمیشن**
- انسانی جسم کے کی پوائنٹس کا پتہ لگانا اور ٹریکنگ
- انسان-روبوٹ تعامل کے لیے اہم
- ایپلیکیشنز: اشاروں کی شناخت، سرگرمی کی سمجھ، محفوظ تعاون

## گہرائی سینسنگ اور 3D ادراک

نیویگیشن اور ہیرا پھیری کے لیے 3D ڈھانچے کو سمجھنا بہت ضروری ہے۔

### LiDAR (Light Detection and Ranging)

LiDAR سینسرز لیزر پلسز خارج کرتے ہیں اور فاصلے کا حساب لگانے کے لیے واپسی کا وقت ماپتے ہیں:

- **2D LiDAR**: ایک پلین میں سکیننگ، نیویگیشن کے لیے عام
- **3D LiDAR**: مکمل 360° پوائنٹ کلاؤڈز، خودمختار گاڑیوں میں استعمال
- **سولڈ سٹیٹ LiDAR**: کوئی حرکت پذیر پرزے نہیں، زیادہ کمپیکٹ اور قابل اعتماد

**پوائنٹ کلاؤڈ پروسیسنگ**
```python
import open3d as o3d
import numpy as np

# پوائنٹ کلاؤڈ لوڈ کریں
pcd = o3d.io.read_point_cloud("environment.pcd")

# کارکردگی کے لیے ڈاؤن سیمپل کریں
pcd_downsampled = pcd.voxel_down_sample(voxel_size=0.05)

# آؤٹ لائرز ہٹائیں
pcd_clean, _ = pcd_downsampled.remove_statistical_outlier(
    nb_neighbors=20, std_ratio=2.0
)

# پلین سیگمینٹیشن (زمین کا پتہ لگانا)
plane_model, inliers = pcd_clean.segment_plane(
    distance_threshold=0.01,
    ransac_n=3,
    num_iterations=1000
)

# زمین کا پلین اور رکاوٹیں نکالیں
ground = pcd_clean.select_by_index(inliers)
obstacles = pcd_clean.select_by_index(inliers, invert=True)
```

### ویژن سے گہرائی کا تخمینہ

**Structure from Motion (SfM)**
- متعدد کیمرہ ویوز سے 3D پوائنٹس کی تثلیث
- کیمرہ موشن اور فیچر ٹریکنگ کی ضرورت
- ویژول SLAM سسٹمز میں استعمال

**مونوکیولر ڈیپتھ ایسٹیمیشن**
- ڈیپ لرننگ ماڈلز سنگل امیجز سے گہرائی کی پیشگوئی کرتے ہیں
- حالیہ ماڈلز: MiDaS, DPT (Dense Prediction Transformer)
- خصوصی ہارڈ ویئر کے بغیر گہرائی کا ادراک ممکن بناتے ہیں

## انرشیل میژرمنٹ یونٹس (IMUs)

IMUs روبوٹ کی حرکت اور سمت کے بارے میں اہم معلومات فراہم کرتے ہیں۔

### اجزاء

1. **ایکسیلیرومیٹرز**: 3 محوروں میں لکیری ایکسیلیریشن ماپتے ہیں
2. **جائروسکوپس**: 3 محوروں میں زاویائی رفتار ماپتے ہیں
3. **میگنیٹومیٹرز**: مطلق سمت کے لیے مقناطیسی فیلڈ ماپتے ہیں (اختیاری)

### سینسر فیوژن

مضبوط حالت کے تخمینے کے لیے IMU ڈیٹا کو دوسرے سینسرز کے ساتھ ملانا:

**کمپلیمینٹری فلٹر**
```python
import numpy as np

class ComplementaryFilter:
    def __init__(self, alpha=0.98):
        self.alpha = alpha  # جائرو بمقابلہ ایکسیلیرومیٹر کے لیے وزن
        self.angle = np.array([0.0, 0.0, 0.0])  # رول، پچ، یا
    
    def update(self, accel, gyro, dt):
        # قلیل مدتی درستگی کے لیے جائروسکوپ انٹیگریٹ کریں
        gyro_angle = self.angle + gyro * dt
        
        # ایکسیلیرومیٹر سے زاویہ کا حساب (طویل مدتی درستگی)
        accel_angle = np.array([
            np.arctan2(accel[1], accel[2]),  # رول
            np.arctan2(-accel[0], np.sqrt(accel[1]**2 + accel[2]**2)),  # پچ
            self.angle[2]  # یا بغیر تبدیلی (میگنیٹومیٹر نہیں)
        ])
        
        # کمپلیمینٹری فلٹر فیوژن
        self.angle = self.alpha * gyro_angle + (1 - self.alpha) * accel_angle
        
        return self.angle
```

## ٹیکٹائل اور فورس سینسنگ

جسمانی تعامل کے لیے رابطے کی قوتوں اور ٹیکسچرز کو محسوس کرنے کی ضرورت ہے۔

### فورس/ٹارک سینسرز

- **6-axis F/T سینسرز**: فورسز (Fx, Fy, Fz) اور ٹارکس (Tx, Ty, Tz) ماپتے ہیں
- **جوائنٹ ٹارک سینسرز**: جوائنٹ ٹارکس کا تخمینہ لگانے کے لیے موٹر کرنٹس مانیٹر کرتے ہیں
- **ایپلیکیشنز**: کمپلائنٹ کنٹرول، محفوظ تعامل، آبجیکٹ وزن کا تخمینہ

### ٹیکٹائل سینسرز

- **ریزسٹو سینسرز**: دباؤ میں مزاحمت تبدیل کرتے ہیں
- **کیپیسیٹو سینسرز**: قربت اور لمس کا پتہ لگاتے ہیں
- **آپٹیکل ٹیکٹائل سینسرز**: سطح کی ڈیفارمیشن مشاہدہ کرنے کے لیے کیمرے استعمال کرتے ہیں (GelSight)

**مثال: فورس کنٹرولڈ گراسپنگ**
```python
def adaptive_grasp(force_sensor, target_force=5.0, max_iterations=100):
    """
    ہدف کی رابطہ قوت برقرار رکھنے کے لیے گرپر فورس ایڈجسٹ کریں
    """
    kp = 0.1  # متناسب گین
    gripper_position = 0.0
    
    for i in range(max_iterations):
        # موجودہ فورس پڑھیں
        current_force = force_sensor.read()
        
        # ایرر کا حساب لگائیں
        error = target_force - current_force
        
        # متناسب کنٹرول
        gripper_position += kp * error
        gripper_position = np.clip(gripper_position, 0, 1)
        
        # گرپر کمانڈ لاگو کریں
        gripper.set_position(gripper_position)
        
        # کنورجنس چیک کریں
        if abs(error) < 0.1:
            break
    
    return gripper_position
```

## پروپریوسیپشن: جسمانی آگاہی

پروپریوسیپشن روبوٹ کی اپنی جسمانی کنفیگریشن کے احساس سے مراد ہے۔

### جوائنٹ انکوڈرز

- **ایبسولوٹ انکوڈرز**: پاور آف ہونے پر بھی پوزیشن کی معلومات برقرار رکھتے ہیں
- **انکریمنٹل انکوڈرز**: ایک ریفرنس پوزیشن سے پلسز گنتے ہیں
- **ریزولوشن**: عام طور پر 12-bit سے 17-bit (4096 سے 131,072 کاؤنٹس فی ریوولوشن)

### اسٹیٹ ایسٹیمیشن

**فارورڈ کائنیمیٹکس**: جوائنٹ اینگلز سے اینڈ ایفیکٹر پوزیشن کا حساب

```python
import numpy as np

def forward_kinematics_2dof(theta1, theta2, L1=1.0, L2=1.0):
    """
    2-DOF پلینر آرم کے لیے فارورڈ کائنیمیٹکس
    
    Args:
        theta1: جوائنٹ 1 کا زاویہ (ریڈینز)
        theta2: جوائنٹ 2 کا زاویہ (ریڈینز)
        L1: لنک 1 کی لمبائی
        L2: لنک 2 کی لمبائی
    
    Returns:
        (x, y): اینڈ ایفیکٹر پوزیشن
    """
    x = L1 * np.cos(theta1) + L2 * np.cos(theta1 + theta2)
    y = L1 * np.sin(theta1) + L2 * np.sin(theta1 + theta2)
    
    return x, y
```

## سینسر فیوژن اور ملٹی موڈل ادراک

حقیقی دنیا کے روبوٹس مضبوط ادراک کے لیے متعدد سینسر موڈیلیٹیز کو یکجا کرتے ہیں۔

### ایکسٹینڈڈ کالمن فلٹر (EKF)

EKF شور والی سینسر پیمائشوں کو ملانے کے لیے وسیع پیمانے پر استعمال ہوتا ہے:

1. **پیشگوئی**: اگلی حالت کی پیشگوئی کے لیے موشن ماڈل استعمال کریں
2. **اپڈیٹ**: سینسر پیمائشوں کا استعمال کرتے ہوئے پیشگوئی درست کریں
3. **فیوژن**: متعدد سینسر ذرائع کو بہترین طریقے سے ملائیں

### جدید طریقے: ڈیپ لرننگ فیوژن

- **ملٹی موڈل نیٹ ورکس**: نیورل نیٹ ورکس میں RGB، گہرائی، اور IMU کو یکجا کریں
- **کراس اٹینشن میکانزمز**: مختلف موڈیلیٹیز کو وزن دینا سیکھیں
- **اینڈ ٹو اینڈ لرننگ**: سینسرز سے ایکشنز تک براہ راست میپنگ

## ادراک کے چیلنجز

حقیقی دنیا کا ادراک متعدد چیلنجز کا سامنا کرتا ہے:

1. **روشنی میں تبدیلیاں**: سائے، چمک، دن/رات کی تبدیلیاں
2. **رکاوٹیں**: اہم خصوصیات کے نظارے کو روکنے والی اشیاء
3. **متحرک ماحول**: حرکت کرنے والے لوگ، گاڑیاں، اور اشیاء
4. **سینسر شور**: تمام سینسرز میں پیمائش کی غیر یقینیت ہے
5. **کمپیوٹیشنل رکاوٹیں**: محدود آن بورڈ پروسیسنگ پاور
6. **لیٹینسی**: ادراک کو ریئل ٹائم کنٹرول کے لیے کافی تیز ہونا چاہیے

## کیس اسٹڈی: Tesla Optimus ادراک کا نظام

Tesla کا Optimus ہیومنائڈ روبوٹ اپنی خودمختار گاڑیوں سے ٹیکنالوجی کا فائدہ اٹھاتا ہے:

- **ویژن بیسڈ**: 360° کوریج فراہم کرنے والے 8 کیمرے
- **نیورل نیٹ ورک پروسیسنگ**: ویژن ٹرانسفارمرز چلانے والی کسٹم FSD چپ
- **کوئی LiDAR نہیں**: مکمل طور پر ویژن پر انحصار (لاگت اور سادگی)
- **آکیوپینسی نیٹ ورکس**: ماحول کی 3D وی آکسل نمائندگی
- **لینگویج گراؤنڈنگ**: ویژن کو قدرتی زبان کی کمانڈز سے جوڑنا

## خلاصہ

روبوٹ ادراک فزیکل AI سسٹمز میں ذہین رویے کی بنیاد ہے۔ اہم نکات:

- بصارت بنیادی حس ہے، جسے گہرائی سینسرز اور IMUs سے بڑھایا گیا ہے
- متعدد سینسرز ریڈنڈینسی اور تکمیلی معلومات فراہم کرتے ہیں
- ڈیپ لرننگ نے ادراک کی صلاحیتوں میں انقلاب برپا کیا ہے
- مضبوط حالت کے تخمینے کے لیے سینسر فیوژن ضروری ہے
- ریئل ٹائم پروسیسنگ اور کارکردگی اہم رکاوٹیں ہیں

اگلے باب میں، ہم دریافت کریں گے کہ روبوٹس ایکچویٹرز اور موومنٹ کنٹرول کے ذریعے اپنے ادراک پر کیسے عمل کرتے ہیں۔

---

**پچھلا**: [تعارف ←](./intro.md) | **اگلا**: [ایکچویٹرز اور موومنٹ کنٹرول →](./chapter2.md)

## مشقیں

1. OpenCV استعمال کرتے ہوئے ایک سادہ آبجیکٹ ٹریکنگ سسٹم نافذ کریں
2. مونوکیولر بمقابلہ سٹیریو گہرائی تخمینے کی درستگی کا موازنہ کریں
3. اندرونی جگہ میں کام کرنے والے ہیومنائڈ روبوٹ کے لیے سینسر سویٹ ڈیزائن کریں
4. IMU اورینٹیشن تخمینے کے لیے کمپلیمینٹری فلٹر نافذ کریں
5. روبوٹ ویژن کے لیے مختلف کیمرہ اقسام کے درمیان تجارتی موازنہ کا تجزیہ کریں
