---
sidebar_position: 5
---

# Ù…Ø§ÚˆÛŒÙˆÙ„ 4: Voice-to-Action (VLA)

> **ØªÙˆØ¬Û: Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Ø§ Ú©Ù†Ù¹Ø±ÙˆÙ„ Ø§ÙˆØ± Vision-Language-Action Ù…Ø§ÚˆÙ„Ø²**

ÛŒÛØ§Úº Ø¬Ø§Ø¯Ùˆ ÛÙˆØªØ§ ÛÛ’Û” Ø§Ø³ Ø¢Ø®Ø±ÛŒ Ù…Ø§ÚˆÛŒÙˆÙ„ Ù…ÛŒÚºØŒ Ø¢Ù¾ Ø§Ù¾Ù†Û’ Ø±ÙˆØ¨ÙˆÙ¹ Ú©Ùˆ Ø§Ù†Ø³Ø§Ù†ÛŒ ØªÙ‚Ø±ÛŒØ± Ø³Ù…Ø¬Ú¾Ù†Û’ØŒ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø³ÙˆÚ†Ù†Û’ØŒ Ø§ÙˆØ± Ù¾ÛŒÚ†ÛŒØ¯Û Ø¹Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÙ†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØª Ø¯ÛŒÚº Ú¯Û’ â€” Ø³Ø¨ Ú©Ú†Ú¾ Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Û’ Ø°Ø±ÛŒØ¹Û’Û”

## VLA Ø§Ù†Ù‚Ù„Ø§Ø¨

Ø±ÙˆØ§ÛŒØªÛŒ Ø±ÙˆØ¨ÙˆÙ¹ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯:
```
if speech == "get water":
    move_to(kitchen)
    find(cup)
    grasp(cup)
    move_to(sink)
    ...
```

VLA Ø§Ù¾Ø±ÙˆÚ†:
```
Ø§Ù†Ø³Ø§Ù†: "Ù…Ø¬Ú¾Û’ Ù¾ÛŒØ§Ø³ Ù„Ú¯ÛŒ ÛÛ’"
Ø±ÙˆØ¨ÙˆÙ¹: *Ø³ÛŒØ§Ù‚ Ø³Ù…Ø¬Ú¾ØªØ§ ÛÛ’ØŒ Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø§ØªØ§ ÛÛ’ØŒ Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒØªØ§ ÛÛ’*
```

VLA Ù…Ø§ÚˆÙ„Ø² ÛŒÚ©Ø¬Ø§ Ú©Ø±ØªÛ’ ÛÛŒÚº:
- **Vision**: Ú©ÛŒÙ…Ø±ÙˆÚº Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¯Ù†ÛŒØ§ Ø¯ÛŒÚ©Ú¾Ù†Ø§
- **Language**: Ù‚Ø¯Ø±ØªÛŒ commands Ø³Ù…Ø¬Ú¾Ù†Ø§
- **Action**: Ù…ÙˆÙ¹Ø± commands generate Ú©Ø±Ù†Ø§

## Architecture Ú©Ø§ Ø¬Ø§Ø¦Ø²Û

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VLA Pipeline                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤ Ø¢ÙˆØ§Ø²    â”‚  ğŸ‘ï¸ Vision   â”‚  ğŸ§  LLM     â”‚  ğŸ¦¾ Action       â”‚
â”‚  Input      â”‚  Input      â”‚  Planner    â”‚  Generation      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Whisper    â”‚  Camera     â”‚  Gemini/    â”‚  Policy          â”‚
â”‚  STT        â”‚  RGB-D      â”‚  GPT-4/etc  â”‚  Network         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Robot Hardware                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Whisper Ú©Û’ Ø³Ø§ØªÚ¾ Ø¢ÙˆØ§Ø² Ú©ÛŒ Input

OpenAI Ú©Ø§ Whisper Ø§ÛŒÚ© Ø¬Ø¯ÛŒØ¯ ØªØ±ÛŒÙ† speech recognition Ù…Ø§ÚˆÙ„ ÛÛ’:

### Ø§Ù†Ø³Ù¹Ø§Ù„ÛŒØ´Ù†

```bash
# Whisper Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº
pip install openai-whisper

# ØªÛŒØ² inference Ú©Û’ Ù„ÛŒÛ’
pip install faster-whisper

# Ø¢ÚˆÛŒÙˆ dependencies Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº
sudo apt install portaudio19-dev python3-pyaudio
pip install pyaudio sounddevice
```

### Real-time Speech Recognition

```python
import whisper
import sounddevice as sd
import numpy as np
from scipy.io.wavfile import write
import tempfile

class VoiceInterface:
    def __init__(self, model_size="base"):
        """Whisper Ù…Ø§ÚˆÙ„ initialize Ú©Ø±ÛŒÚº
        
        Ù…Ø§ÚˆÙ„ Ø³Ø§Ø¦Ø²: tinyØŒ baseØŒ smallØŒ mediumØŒ large
        Ø¨Ú‘Ø§ = Ø²ÛŒØ§Ø¯Û Ø¯Ø±Ø³ØªØŒ Ø³Ø³Øª
        """
        self.model = whisper.load_model(model_size)
        self.sample_rate = 16000
        
    def listen(self, duration=5):
        """Ù…Ø®ØµÙˆØµ Ù…Ø¯Øª Ú©Û’ Ù„ÛŒÛ’ Ø¢ÚˆÛŒÙˆ Ø±ÛŒÚ©Ø§Ø±Úˆ Ú©Ø±ÛŒÚº"""
        print("Ø³Ù† Ø±ÛØ§ ÛÙˆÚº...")
        
        # Ø¢ÚˆÛŒÙˆ Ø±ÛŒÚ©Ø§Ø±Úˆ Ú©Ø±ÛŒÚº
        audio = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype='float32'
        )
        sd.wait()
        
        return audio.flatten()
        
    def transcribe(self, audio):
        """Ø¢ÚˆÛŒÙˆ Ú©Ùˆ Ù…ØªÙ† Ù…ÛŒÚº Ø¨Ø¯Ù„ÛŒÚº"""
        # Whisper float32 numpy array Ú†Ø§ÛØªØ§ ÛÛ’
        result = self.model.transcribe(
            audio,
            language="en",
            fp16=False  # GPU Ù¾Ø± fp16=True Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
        )
        return result["text"].strip()
        
    def listen_and_transcribe(self, duration=5):
        """Ø§ÛŒÚ© Ø¨Ø§Ø± Ù…ÛŒÚº Ø¢ÙˆØ§Ø² Ø³Û’ Ù…ØªÙ†"""
        audio = self.listen(duration)
        text = self.transcribe(audio)
        print(f"Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: {text}")
        return text

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
voice = VoiceInterface(model_size="base")
command = voice.listen_and_transcribe(duration=5)
```

### Wake Word Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø³Ù„Ø³Ù„ Ø³Ù†Ù†Ø§

```python
import threading
from queue import Queue

class ContinuousListener:
    def __init__(self, wake_word="hey robot"):
        self.voice = VoiceInterface(model_size="small")
        self.wake_word = wake_word.lower()
        self.command_queue = Queue()
        self.running = False
        
    def start(self):
        """Background thread Ù…ÛŒÚº Ø³Ù†Ù†Ø§ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº"""
        self.running = True
        self.thread = threading.Thread(target=self._listen_loop)
        self.thread.start()
        
    def stop(self):
        self.running = False
        self.thread.join()
        
    def _listen_loop(self):
        while self.running:
            # Wake word Ú©Û’ Ù„ÛŒÛ’ Ø³Ù†ÛŒÚº (Ú†Ú¾ÙˆÙ¹Û’ clips)
            audio = self.voice.listen(duration=2)
            text = self.voice.transcribe(audio)
            
            if self.wake_word in text.lower():
                print("Wake word Ù…Ù„Ø§! Command Ú©Û’ Ù„ÛŒÛ’ Ø³Ù† Ø±ÛØ§ ÛÙˆÚº...")
                
                # Ø§ØµÙ„ command Ú©Û’ Ù„ÛŒÛ’ Ø³Ù†ÛŒÚº (Ù„Ù…Ø¨Ø§)
                audio = self.voice.listen(duration=5)
                command = self.voice.transcribe(audio)
                
                if command:
                    self.command_queue.put(command)
                    
    def get_command(self, timeout=None):
        """Queue Ø³Û’ Ø§Ú¯Ù„Ø§ command Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº"""
        try:
            return self.command_queue.get(timeout=timeout)
        except:
            return None

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
listener = ContinuousListener(wake_word="hey robot")
listener.start()

while True:
    command = listener.get_command(timeout=1)
    if command:
        print(f"Command process Ú©Ø± Ø±ÛØ§ ÛÙˆÚº: {command}")
        # LLM planner Ú©Ùˆ Ø¨Ú¾ÛŒØ¬ÛŒÚº
```

## LLM Task Planning

Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Ùˆ Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒÚº Ø¨Ø¯Ù„Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ LLM Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:

### Google Gemini Ú©Û’ Ø³Ø§ØªÚ¾ Task Planner

```python
import google.generativeai as genai
import json

class TaskPlanner:
    def __init__(self):
        genai.configure(api_key="YOUR_GEMINI_API_KEY")
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
        # Ø¯Ø³ØªÛŒØ§Ø¨ Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø¹Ù…Ø§Ù„ Ø¨ÛŒØ§Ù† Ú©Ø±ÛŒÚº
        self.actions = """
        Ø¯Ø³ØªÛŒØ§Ø¨ Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø¹Ù…Ø§Ù„:
        - move_to(location: str) - Ú©Ø³ÛŒ Ø¬Ú¯Û navigate Ú©Ø±ÛŒÚº
        - pick_up(object: str) - Ú©ÙˆØ¦ÛŒ Ú†ÛŒØ² Ù¾Ú©Ú‘ÛŒÚº Ø§ÙˆØ± Ø§Ù¹Ú¾Ø§Ø¦ÛŒÚº
        - put_down(surface: str) - Ù¾Ú©Ú‘ÛŒ ÛÙˆØ¦ÛŒ Ú†ÛŒØ² Ú©ÛÛŒÚº Ø±Ú©Ú¾ÛŒÚº
        - look_at(target: str) - Ú©ÛŒÙ…Ø±Ø§ target Ú©ÛŒ Ø·Ø±Ù Ù…ÙˆÚ‘ÛŒÚº
        - say(message: str) - Ù¾ÛŒØºØ§Ù… Ø¨ÙˆÙ„ÛŒÚº
        - wait(seconds: float) - Ø¹Ù…Ù„ Ø±ÙˆÚ©ÛŒÚº
        - find(object: str) -> bool - Ú†ÛŒØ² ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº
        - is_holding() -> bool - Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ú©Ú†Ú¾ Ù¾Ú©Ú‘Ø§ ÛÙˆØ§ ÛÛ’
        """
        
    def plan(self, command: str, context: dict = None) -> list:
        """Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Ùˆ Ø¹Ù…Ù„ Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ù…ÛŒÚº Ø¨Ø¯Ù„ÛŒÚº"""
        
        prompt = f"""Ø¢Ù¾ Ø§ÛŒÚ© Ø±ÙˆØ¨ÙˆÙ¹ Ù¹Ø§Ø³Ú© Ù¾Ù„Ø§Ù†Ø± ÛÛŒÚºÛ” ØµØ§Ø±Ù Ú©ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ùˆ 
Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø¹Ù…Ø§Ù„ Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ù…ÛŒÚº Ø¨Ø¯Ù„ÛŒÚºÛ”

{self.actions}

Ù…ÙˆØ¬ÙˆØ¯Û Ø³ÛŒØ§Ù‚:
- Ø±ÙˆØ¨ÙˆÙ¹ Ú©ÛŒ Ø¬Ú¯Û: {context.get('location', 'Ù„ÙˆÙ†Ú¯ Ø±ÙˆÙ…')}
- Ù†Ø¸Ø± Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ Ú†ÛŒØ²ÛŒÚº: {context.get('visible_objects', [])}
- ÙÛŒ Ø§Ù„Ø­Ø§Ù„ Ù¾Ú©Ú‘Ø§ ÛÙˆØ§: {context.get('holding', None)}

ØµØ§Ø±Ù Ú©Ø§ Ø­Ú©Ù…: "{command}"

Ø§Ø¹Ù…Ø§Ù„ Ú©ÛŒ JSON array ÙˆØ§Ù¾Ø³ Ú©Ø±ÛŒÚºÛ” ÛØ± Ø¹Ù…Ù„ Ù…ÛŒÚº:
- "action": ÙÙ†Ú©Ø´Ù† Ú©Ø§ Ù†Ø§Ù…
- "params": Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©ÛŒ dictionary
- "reasoning": ÛŒÛ Ø¹Ù…Ù„ Ú©ÛŒÙˆÚº Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’

Ù…Ø«Ø§Ù„ Ø¬ÙˆØ§Ø¨:
[
  {{"action": "move_to", "params": {{"location": "kitchen"}}, "reasoning": "Ù¾ÛÙ„Û’ Ú©Ú†Ù† Ø¬Ø§Ù†Ø§ ÛÛ’"}},
  {{"action": "find", "params": {{"object": "cup"}}, "reasoning": "Ú©Ù¾ ÚˆÚ¾ÙˆÙ†Úˆ Ø±ÛØ§ ÛÙˆÚº"}}
]
"""
        
        response = self.model.generate_content(prompt)
        
        # Ø¬ÙˆØ§Ø¨ Ø³Û’ JSON parse Ú©Ø±ÛŒÚº
        text = response.text
        # Ø¬ÙˆØ§Ø¨ Ø³Û’ JSON array Ù†Ú©Ø§Ù„ÛŒÚº
        start = text.find('[')
        end = text.rfind(']') + 1
        json_str = text[start:end]
        
        return json.loads(json_str)
        
    def execute_plan(self, plan: list, robot):
        """Ø±ÙˆØ¨ÙˆÙ¹ Ù¾Ø± Ù…Ù†ØµÙˆØ¨Û Ú†Ù„Ø§Ø¦ÛŒÚº"""
        for step in plan:
            action = step['action']
            params = step['params']
            
            print(f"Ú†Ù„Ø§ Ø±ÛØ§ ÛÙˆÚº: {action}({params})")
            
            # Ù…Ù†Ø§Ø³Ø¨ Ø±ÙˆØ¨ÙˆÙ¹ method call Ú©Ø±ÛŒÚº
            method = getattr(robot, action)
            result = method(**params)
            
            if not result:
                print(f"Ø¹Ù…Ù„ {action} Ù†Ø§Ú©Ø§Ù… ÛÙˆØ§!")
                return False
                
        return True

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
planner = TaskPlanner()
context = {
    'location': 'Ù„ÙˆÙ†Ú¯ Ø±ÙˆÙ…',
    'visible_objects': ['ØµÙˆÙÛ', 'Ù…ÛŒØ²', 'Ø±ÛŒÙ…ÙˆÙ¹'],
    'holding': None
}

plan = planner.plan("Ø¨Ø±Ø§Û Ú©Ø±Ù… Ù…Ø¬Ú¾Û’ Ù¾Ø§Ù†ÛŒ Ú©Ø§ Ú¯Ù„Ø§Ø³ Ù„Ø§ Ø¯ÛŒÚº", context)
print(json.dumps(plan, indent=2))
```

### Chain-of-Thought Reasoning

Ù¾ÛŒÚ†ÛŒØ¯Û Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ØŒ chain-of-thought prompting Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:

```python
class AdvancedPlanner:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-pro')
        
    def plan_with_reasoning(self, command: str, context: dict) -> dict:
        prompt = f"""Ø¢Ù¾ Ø§ÛŒÚ© Ø°ÛÛŒÙ† Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ” Ø§Ø³ Ú©Ø§Ù… Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº 
Ù‚Ø¯Ù… Ø¨Û Ù‚Ø¯Ù… Ø³ÙˆÚ†ÛŒÚºÛ”

Ú©Ø§Ù…: {command}

Ø³ÛŒØ§Ù‚:
- Ù…ÙˆØ¬ÙˆØ¯Û Ø¬Ú¯Û: {context.get('location')}
- Ù…Ø¹Ù„ÙˆÙ… Ø¬Ú¯ÛÛŒÚº: {context.get('known_locations', [])}
- Ù†Ø¸Ø± Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ Ú†ÛŒØ²ÛŒÚº: {context.get('visible_objects', [])}
- Ù¾Ú©Ú‘Ø§ ÛÙˆØ§: {context.get('holding')}

Ù‚Ø¯Ù… Ø¨Û Ù‚Ø¯Ù… Ø³ÙˆÚ†ÛŒÚº:
1. ØµØ§Ø±Ù Ø§ØµÙ„ Ù…ÛŒÚº Ú©ÛŒØ§ Ú†Ø§ÛØªØ§ ÛÛ’ØŸ
2. Ù…Ø¬Ú¾Û’ Ú©ÙˆÙ† Ø³ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú†Ø§ÛÛŒÛ’ØŸ
3. Ú©ÛŒØ§ ØºÙ„Ø· ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ØŸ
4. Ø§Ø¹Ù…Ø§Ù„ Ú©ÛŒ Ø¨ÛØªØ±ÛŒÙ† ØªØ±ØªÛŒØ¨ Ú©ÛŒØ§ ÛÛ’ØŸ

Ø§Ù¾Ù†Ø§ Ø¬ÙˆØ§Ø¨ Ø§Ø³ ÙØ§Ø±Ù…ÛŒÙ¹ Ù…ÛŒÚº Ø¯ÛŒÚº:
{{
  "understanding": "Ù…ÛŒÚº Ø³Ù…Ø¬Ú¾ØªØ§ ÛÙˆÚº ØµØ§Ø±Ù Ú©ÛŒØ§ Ú†Ø§ÛØªØ§ ÛÛ’",
  "reasoning": "Ù‚Ø¯Ù… Ø¨Û Ù‚Ø¯Ù… Ø³ÙˆÚ†",
  "preconditions": ["Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ Ú©ÛŒØ§ Ø³Ú† ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’"],
  "plan": [
    {{"action": "...", "params": {{}}, "fallback": "..."}}
  ],
  "success_criteria": "Ú©ÛŒØ³Û’ Ù¾ØªÛ Ú†Ù„Û’ Ú©Û Ú©Ø§Ù… Ú©Ø§Ù…ÛŒØ§Ø¨ ÛÙˆØ§"
}}
"""
        response = self.model.generate_content(prompt)
        return json.loads(response.text)
```

## Vision-Language Ù…Ø§ÚˆÙ„Ø²

Ø³Ù…Ø¬Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ vision Ø§ÙˆØ± language ÛŒÚ©Ø¬Ø§ Ú©Ø±ÛŒÚº:

### Scene Understanding

```python
import google.generativeai as genai
from PIL import Image
import base64
import io

class VisionLanguageModel:
    def __init__(self):
        genai.configure(api_key="YOUR_API_KEY")
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
    def describe_scene(self, image_path: str) -> str:
        """Ø±ÙˆØ¨ÙˆÙ¹ Ú©ÛŒØ§ Ø¯ÛŒÚ©Ú¾ØªØ§ ÛÛ’ Ø§Ø³Û’ Ø¨ÛŒØ§Ù† Ú©Ø±ÛŒÚº"""
        image = Image.open(image_path)
        
        response = self.model.generate_content([
            "Ø±ÙˆØ¨ÙˆÙ¹ Ú©Û’ Ù†Ù‚Ø·Û Ù†Ø¸Ø± Ø³Û’ Ø§Ø³ Ù…Ù†Ø¸Ø± Ú©Ùˆ Ø¨ÛŒØ§Ù† Ú©Ø±ÛŒÚºÛ” Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº:",
            "1. Ù†Ø¸Ø± Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ Ú†ÛŒØ²ÛŒÚº Ø§ÙˆØ± Ø§Ù† Ú©ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ù¾ÙˆØ²ÛŒØ´Ù†Ø²",
            "2. Ú©ÙˆØ¦ÛŒ Ù„ÙˆÚ¯ Ø§ÙˆØ± ÙˆÛ Ú©ÛŒØ§ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº",
            "3. Ù…Ù…Ú©Ù†Û navigation Ø±Ø§Ø³ØªÛ’",
            "4. Ú©ÙˆØ¦ÛŒ Ø®Ø·Ø±Ø§Øª ÛŒØ§ Ø±Ú©Ø§ÙˆÙ¹ÛŒÚº",
            image
        ])
        
        return response.text
        
    def find_object(self, image_path: str, target: str) -> dict:
        """Ù…Ù†Ø¸Ø± Ù…ÛŒÚº Ù…Ø®ØµÙˆØµ Ú†ÛŒØ² ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº"""
        image = Image.open(image_path)
        
        response = self.model.generate_content([
            f"Ø§Ø³ ØªØµÙˆÛŒØ± Ù…ÛŒÚº {target} ÚˆÚ¾ÙˆÙ†ÚˆÛŒÚºÛ”",
            "Ø§Ú¯Ø± Ù…Ù„Ø§ ØªÙˆØŒ Ø§Ø³ Ú©ÛŒ Ø¬Ú¯Û Ø¨ÛŒØ§Ù† Ú©Ø±ÛŒÚº (Ø¨Ø§Ø¦ÛŒÚº/Ø¯Ø±Ù…ÛŒØ§Ù†/Ø¯Ø§Ø¦ÛŒÚºØŒ Ù‚Ø±ÛŒØ¨/Ø¯ÙˆØ±)",
            "Ø§Ú¯Ø± Ù†ÛÛŒÚº Ù…Ù„Ø§ ØªÙˆØŒ 'Ù†Ø¸Ø± Ù†ÛÛŒÚº Ø¢ØªØ§' Ú©ÛÛŒÚº",
            'JSON Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº: {"found": bool, "location": str, "confidence": float}',
            image
        ])
        
        return json.loads(response.text)
        
    def answer_question(self, image_path: str, question: str) -> str:
        """Ø±ÙˆØ¨ÙˆÙ¹ Ú©ÛŒØ§ Ø¯ÛŒÚ©Ú¾ØªØ§ ÛÛ’ Ø§Ø³ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº"""
        image = Image.open(image_path)
        
        response = self.model.generate_content([
            question,
            image
        ])
        
        return response.text

# ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„
class VisionNode(Node):
    def __init__(self):
        super().__init__('vision_node')
        self.vlm = VisionLanguageModel()
        self.bridge = CvBridge()
        
        self.image_sub = self.create_subscription(
            Image, '/camera/image', self.image_callback, 10
        )
        
        self.query_srv = self.create_service(
            StringQuery, '/vision/query', self.query_callback
        )
        
    def image_callback(self, msg):
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")
        
    def query_callback(self, request, response):
        # Ù…ÙˆØ¬ÙˆØ¯Û frame Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº
        cv2.imwrite('/tmp/current_frame.jpg', self.latest_image)
        
        # VLM Ø³Û’ Ù¾ÙˆÚ†Ú¾ÛŒÚº
        answer = self.vlm.answer_question(
            '/tmp/current_frame.jpg',
            request.query
        )
        
        response.answer = answer
        return response
```

## Ù…Ú©Ù…Ù„ VLA Ø³Ø³Ù¹Ù…

Ø³Ø¨ Ú©Ú†Ú¾ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾:

```python
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist, PoseStamped
from sensor_msgs.msg import Image
from std_msgs.msg import String
import threading

class VLARobot(Node):
    def __init__(self):
        super().__init__('vla_robot')
        
        # Ø§Ø¬Ø²Ø§Ø¡ initialize Ú©Ø±ÛŒÚº
        self.voice = VoiceInterface(model_size="base")
        self.planner = TaskPlanner()
        self.vision = VisionLanguageModel()
        
        # ROS 2 publishers/subscribers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.speech_pub = self.create_publisher(String, '/speech', 10)
        self.nav_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)
        
        self.image_sub = self.create_subscription(
            Image, '/camera/image', self.image_callback, 10
        )
        
        # State
        self.current_image = None
        self.context = {
            'location': 'Ú¯Ú¾Ø±',
            'holding': None,
            'visible_objects': []
        }
        
        # Voice listener Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº
        self.voice_thread = threading.Thread(target=self.voice_loop)
        self.voice_thread.start()
        
    def voice_loop(self):
        """Ù…Ø³Ù„Ø³Ù„ commands Ú©Û’ Ù„ÛŒÛ’ Ø³Ù†ÛŒÚº"""
        while rclpy.ok():
            try:
                command = self.voice.listen_and_transcribe(duration=5)
                if command and len(command) > 3:
                    self.process_command(command)
            except Exception as e:
                self.get_logger().error(f"Ø¢ÙˆØ§Ø² Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ: {e}")
                
    def process_command(self, command: str):
        """Ø¢ÙˆØ§Ø² Ú©Ø§ Ø­Ú©Ù… process Ú©Ø±ÛŒÚº"""
        self.get_logger().info(f"Process Ú©Ø± Ø±ÛØ§ ÛÙˆÚº: {command}")
        
        # Ù…ÙˆØ¬ÙˆØ¯Û vision Ú©Û’ Ø³Ø§ØªÚ¾ context update Ú©Ø±ÛŒÚº
        if self.current_image is not None:
            scene_description = self.vision.describe_scene(self.current_image)
            self.context['visible_objects'] = self._extract_objects(scene_description)
        
        # LLM Ø³Û’ Ù…Ù†ØµÙˆØ¨Û Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
        plan = self.planner.plan(command, self.context)
        
        # ØµØ§Ø±Ù Ø³Û’ ØªØµØ¯ÛŒÙ‚ Ú©Ø±ÛŒÚº
        self.say(f"Ù…ÛŒÚº {self._summarize_plan(plan)} Ú©Ø±ÙˆÚº Ú¯Ø§Û” Ù¹Ú¾ÛŒÚ© ÛÛ’ØŸ")
        
        # ØªØµØ¯ÛŒÙ‚ Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø±ÛŒÚº
        response = self.voice.listen_and_transcribe(duration=3)
        if "ÛØ§Úº" in response.lower() or "Ù¹Ú¾ÛŒÚ©" in response.lower():
            self.execute_plan(plan)
        else:
            self.say("Ù¹Ú¾ÛŒÚ© ÛÛ’ØŒ Ù…ÛŒÚº ÛŒÛ Ù†ÛÛŒÚº Ú©Ø±ÙˆÚº Ú¯Ø§Û”")
            
    def execute_plan(self, plan: list):
        """Ø¹Ù…Ù„ Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ú†Ù„Ø§Ø¦ÛŒÚº"""
        for step in plan:
            action = step['action']
            params = step['params']
            
            self.get_logger().info(f"Ú†Ù„Ø§ Ø±ÛØ§ ÛÙˆÚº: {action}")
            
            if action == 'move_to':
                self.move_to(params['location'])
            elif action == 'pick_up':
                self.pick_up(params['object'])
            elif action == 'put_down':
                self.put_down(params['surface'])
            elif action == 'say':
                self.say(params['message'])
            elif action == 'find':
                found = self.find(params['object'])
                if not found:
                    self.say(f"Ù…Ø¬Ú¾Û’ {params['object']} Ù†ÛÛŒÚº Ù…Ù„Ø§")
                    return
            elif action == 'wait':
                time.sleep(params['seconds'])
                
    # Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø¹Ù…Ø§Ù„
    def move_to(self, location: str):
        """Ú©Ø³ÛŒ Ø¬Ú¯Û navigate Ú©Ø±ÛŒÚº"""
        # Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø± ÛŒÛ Nav2 Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Û’ Ú¯Ø§
        pose = PoseStamped()
        pose.header.frame_id = 'map'
        pose.pose.position.x = self._get_location_coords(location)[0]
        pose.pose.position.y = self._get_location_coords(location)[1]
        self.nav_pub.publish(pose)
        
    def say(self, message: str):
        """Ù¾ÛŒØºØ§Ù… Ø¨ÙˆÙ„ÛŒÚº (TTS)"""
        msg = String()
        msg.data = message
        self.speech_pub.publish(msg)
        
    def find(self, object_name: str) -> bool:
        """Ú©ÙˆØ¦ÛŒ Ú†ÛŒØ² ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº"""
        if self.current_image is None:
            return False
            
        result = self.vision.find_object(self.current_image, object_name)
        return result.get('found', False)
        
    def pick_up(self, object_name: str):
        """Ú©ÙˆØ¦ÛŒ Ú†ÛŒØ² Ø§Ù¹Ú¾Ø§Ø¦ÛŒÚº (arm Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ø±Û’ Ú¯Ø§)"""
        self.get_logger().info(f"{object_name} Ø§Ù¹Ú¾Ø§ Ø±ÛØ§ ÛÙˆÚº")
        self.context['holding'] = object_name
        
    def put_down(self, surface: str):
        """Ù¾Ú©Ú‘ÛŒ ÛÙˆØ¦ÛŒ Ú†ÛŒØ² Ø±Ú©Ú¾ÛŒÚº"""
        self.get_logger().info(f"{surface} Ù¾Ø± Ø±Ú©Ú¾ Ø±ÛØ§ ÛÙˆÚº")
        self.context['holding'] = None
        
    def image_callback(self, msg):
        """ØªØ§Ø²Û ØªØ±ÛŒÙ† camera frame Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº"""
        self.current_image = msg
        
    def _get_location_coords(self, location: str) -> tuple:
        """Ø¬Ú¯Û Ú©Û’ Ù†Ø§Ù…ÙˆÚº Ú©Ùˆ coordinates Ù…ÛŒÚº map Ú©Ø±ÛŒÚº"""
        locations = {
            'Ú©Ú†Ù†': (5.0, 2.0),
            'Ù„ÙˆÙ†Ú¯ Ø±ÙˆÙ…': (0.0, 0.0),
            'Ø¨ÛŒÚˆ Ø±ÙˆÙ…': (-3.0, 4.0),
            'Ø¨Ø§ØªÚ¾ Ø±ÙˆÙ…': (-3.0, 0.0)
        }
        return locations.get(location, (0.0, 0.0))
        
    def _summarize_plan(self, plan: list) -> str:
        """Ø§Ù†Ø³Ø§Ù† Ú©Û’ Ù¾Ú‘Ú¾Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§Ø¦ÛŒÚº"""
        actions = [f"{s['action']} {list(s['params'].values())[0] if s['params'] else ''}" 
                   for s in plan[:3]]
        return "ØŒ Ù¾Ú¾Ø± ".join(actions)

def main():
    rclpy.init()
    robot = VLARobot()
    rclpy.spin(robot)

if __name__ == '__main__':
    main()
```

## Ø±ÙˆØ¨ÙˆÙ¹ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ù„ÛŒÛ’ Text-to-Speech

Ø±ÙˆØ¨ÙˆÙ¹ Ú©Ùˆ Ø¨ÙˆÙ„Ù†Û’ Ø¯ÛŒÚº:

```python
import pyttsx3
# ÛŒØ§ Ø¨ÛØªØ± Ù…Ø¹ÛŒØ§Ø± Ú©Û’ Ù„ÛŒÛ’ cloud TTS Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:
from gtts import gTTS
import pygame

class SpeechOutput:
    def __init__(self, engine='local'):
        if engine == 'local':
            self.engine = pyttsx3.init()
            self.engine.setProperty('rate', 150)
        else:
            pygame.mixer.init()
            
        self.engine_type = engine
        
    def speak(self, text: str):
        """Ù…ØªÙ† Ú©Ùˆ Ø¢ÙˆØ§Ø² Ù…ÛŒÚº Ø¨Ø¯Ù„ÛŒÚº"""
        if self.engine_type == 'local':
            self.engine.say(text)
            self.engine.runAndWait()
        else:
            # Cloud TTS (Ø¨ÛØªØ± Ù…Ø¹ÛŒØ§Ø±)
            tts = gTTS(text=text, lang='ur')  # Ø§Ø±Ø¯Ùˆ Ú©Û’ Ù„ÛŒÛ’
            tts.save('/tmp/speech.mp3')
            pygame.mixer.music.load('/tmp/speech.mp3')
            pygame.mixer.music.play()
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)

# ROS 2 TTS Node
class TTSNode(Node):
    def __init__(self):
        super().__init__('tts_node')
        self.tts = SpeechOutput(engine='cloud')
        
        self.subscription = self.create_subscription(
            String,
            '/speech',
            self.speech_callback,
            10
        )
        
    def speech_callback(self, msg):
        self.tts.speak(msg.data)
```

## Capstone Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹: Home Assistant Ø±ÙˆØ¨ÙˆÙ¹

Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ home assistant Ø±ÙˆØ¨ÙˆÙ¹ Ø¨Ù†Ø§Ø¦ÛŒÚº Ø¬Ùˆ:

1. Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Û’ commands Ú©Û’ Ù„ÛŒÛ’ **Ø³Ù†Û’**
2. Ù…Ø§Ø­ÙˆÙ„ Ú©Ùˆ **Ø¯ÛŒÚ©Ú¾Û’** Ø§ÙˆØ± Ø³Ù…Ø¬Ú¾Û’
3. Ù¾ÛŒÚ†ÛŒØ¯Û multi-step Ú©Ø§Ù… **Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ** Ú©Ø±Û’
4. Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ **navigate** Ú©Ø±Û’
5. Ø§Ù†Ø³Ø§Ù†ÙˆÚº Ø³Û’ communicate Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ **Ø¨ÙˆÙ„Û’**

### Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ú©Ø§ ÚˆÚ¾Ø§Ù†Ú†Û

```
home_assistant_robot/
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ full_system.launch.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ nav2_params.yaml
â”‚   â””â”€â”€ locations.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ voice_interface.py
â”‚   â”œâ”€â”€ task_planner.py
â”‚   â”œâ”€â”€ vision_system.py
â”‚   â””â”€â”€ robot_controller.py
â””â”€â”€ package.xml
```

### Integration Launch File

```python
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Navigation stack
        IncludeLaunchDescription(
            PythonLaunchDescriptionSource([
                FindPackageShare('nav2_bringup'),
                '/launch/navigation_launch.py'
            ])
        ),
        
        # Isaac ROS VSLAM
        Node(
            package='isaac_ros_visual_slam',
            executable='visual_slam_node',
            name='vslam'
        ),
        
        # Voice interface
        Node(
            package='home_assistant_robot',
            executable='voice_interface.py',
            name='voice_interface'
        ),
        
        # Task planner
        Node(
            package='home_assistant_robot',
            executable='task_planner.py',
            name='task_planner'
        ),
        
        # Vision system
        Node(
            package='home_assistant_robot',
            executable='vision_system.py',
            name='vision_system'
        ),
        
        # Text-to-speech
        Node(
            package='home_assistant_robot',
            executable='tts_node.py',
            name='tts'
        ),
        
        # Main robot controller
        Node(
            package='home_assistant_robot',
            executable='robot_controller.py',
            name='robot_controller'
        )
    ])
```

## Ø¬Ø§Ø¦Ø²Û Ù…Ø¹ÛŒØ§Ø±

Ø¢Ù¾ Ú©Û’ capstone Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û ÛÙˆÚ¯Ø§:

| Ù…Ø¹ÛŒØ§Ø± | Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³ | ØªÙØµÛŒÙ„ |
|-------|---------|-------|
| Voice Recognition | 20 | Ø¯Ø±Ø³Øª speech-to-text |
| Task Planning | 25 | Ù…Ø¹Ù‚ÙˆÙ„ Ø¹Ù…Ù„ Ú©ÛŒ ØªØ±ØªÛŒØ¨ |
| Vision Integration | 20 | Scene understanding |
| Navigation | 20 | Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± Ø­Ø±Ú©Øª |
| Communication | 15 | Ù‚Ø¯Ø±ØªÛŒ Ø¢ÙˆØ§Ø² Ú©ÛŒ output |

## Ø§ÛÙ… Ù†Ú©Ø§Øª

1. **Ø¢ÙˆØ§Ø² Ú©Û’ Ù„ÛŒÛ’ Whisper** - Ø¨ÛØªØ±ÛŒÙ† Ø¯Ø±Ø¬Û’ Ú©ÛŒ speech recognition
2. **Planning Ú©Û’ Ù„ÛŒÛ’ LLMs** - Ø²Ø¨Ø§Ù† Ú©Ùˆ Ø¹Ù…Ù„ Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ù…ÛŒÚº Ø¨Ø¯Ù„ÛŒÚº
3. **Vision Ú©Û’ Ù„ÛŒÛ’ VLMs** - Ø±ÙˆØ¨ÙˆÙ¹ Ú©ÛŒØ§ Ø¯ÛŒÚ©Ú¾ØªØ§ ÛÛ’ Ø³Ù…Ø¬Ú¾ÛŒÚº
4. **Integration Ú©Ù„ÛŒØ¯ÛŒ ÛÛ’** - ØªÙ…Ø§Ù… modalities Ú©Ùˆ Ø¨Û’ ØªÚ©Ù„Ù ÛŒÚ©Ø¬Ø§ Ú©Ø±ÛŒÚº
5. **Feedback loops** - Ø±ÙˆØ¨ÙˆÙ¹ Ú©Ùˆ ÙˆØ§Ù¾Ø³ communicate Ú©Ø±Ù†Û’ Ø¯ÛŒÚº

## Ú©ÙˆØ±Ø³ Ù…Ú©Ù…Ù„! ğŸ‰

Ø¢Ù¾ Ù†Û’ Ø³ÛŒÚ©Ú¾Ø§:
- âœ… ROS 2 Ø±ÙˆØ¨ÙˆÙ¹Ú© Ø³Ø³Ù¹Ù…Ø²
- âœ… Gazebo Ù…ÛŒÚº simulated environments
- âœ… Isaac Ú©Û’ Ø³Ø§ØªÚ¾ GPU-accelerated perception
- âœ… VLA Ú©Û’ Ø³Ø§ØªÚ¾ voice-controlled Ø±ÙˆØ¨ÙˆÙ¹Ø³

**Ø¢Ú¯Û’ Ú©ÛŒØ§ØŸ**
- [ROS 2 Discord](https://discord.gg/ros) Ù…ÛŒÚº Ø´Ø§Ù…Ù„ ÛÙˆÚº
- Open-source Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ Ù…ÛŒÚº Ø­ØµÛ ÚˆØ§Ù„ÛŒÚº
- Ø§Ù¾Ù†Û’ Physical AI Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹Ø³ Ø¨Ù†Ø§Ø¦ÛŒÚº!

---

**ÙˆØ§Ù¾Ø³**: [Ú©ÙˆØ±Ø³ ØªØ¹Ø§Ø±Ù â†’](./intro)
